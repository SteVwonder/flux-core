#!/usr/bin/env python

from __future__ import print_function
import argparse
import re
import csv
import six
import sys
import json
import logging
import heapq
from abc import ABCMeta, abstractmethod
from datetime import datetime, timedelta
from itertools import groupby
from operator import attrgetter
from collections import defaultdict, Sequence

import flux
from flux import job, rpc, util, kvs
from flux.rpc import RPC


def create_resource(res_type, count, with_child=[]):
    assert isinstance(with_child, Sequence), "child resource must be a sequence"
    assert not isinstance(with_child, str), "child resource must not be a string"
    assert count > 0, "resource count must be > 0"

    res = {"type": res_type, "count": count}

    if len(with_child) > 0:
        res["with"] = with_child
    return res


def create_slot(label, count, with_child):
    slot = create_resource("slot", count, with_child)
    slot["label"] = label
    return slot


class Job(object):
    def __init__(self, nnodes, ncpus, submit_time, elapsed_time, timelimit, exitcode=0):
        self.nnodes = nnodes
        self.ncpus = ncpus
        self.submit_time = submit_time
        self.elapsed_time = elapsed_time
        self.timelimit = timelimit
        self.exitcode = exitcode
        self.start_time = None
        self._jobid = None
        self._jobspec = None
        self._submit_future = None

    @property
    def jobspec(self):
        if self._jobspec is not None:
            return self._jobspec

        assert self.ncpus % self.nnodes == 0
        core = create_resource("core", self.ncpus / self.nnodes)
        slot = create_slot("task", 1, [core])
        if self.nnodes > 0:
            resource_section = create_resource("node", self.nnodes, [slot])
        else:
            resource_section = slot

        jobspec = {
            "version": 1,
            "resources": [resource_section],
            "tasks": [
                {
                    "command": ["sleep", "0"],
                    "slot": "task",
                    "count": {"per_slot": 1},
                    "attributes": {},
                }
            ],
            "attributes": {"system": {"duration": self.timelimit}},
        }

        self._jobspec = jobspec
        return self._jobspec

    def submit(self):
        logger.debug("Submitting a new job")
        jobspec_json = json.dumps(self.jobspec)
        logger.debug(jobspec_json)
        self._submit_future = flux.job.submit_async(flux_handle, jobspec_json)

    @property
    def jobid(self):
        if self._jobid is None:
            if self._submit_future is None:
                raise ValueError("Job was not submitted yet. No ID assigned.")
            logger.debug("Waiting on jobid")
            self._jobid = flux.job.submit_get_id(self._submit_future)
            logger.debug("Received jobid: {}".format(self._jobid))
        return self._jobid

    @property
    def complete_time(self):
        if self.start_time is None:
            raise ValueError("Job has not started yet")
        return self.start_time + self.elapsed

    def complete(self):
        raise NotImplementedError()

    def cancel(self):
        raise NotImplementedError()

    def insert_apriori_events(self, simulation):
        # TODO: add priority to `add_event` so that all submits for a given time
        # can happen consecutively, followed by the waits for the jobids
        simulation.add_event(self.submit_time, self.submit)
        simulation.add_event(self.submit_time, lambda: simulation.add_job(self))


class EventList(six.Iterator):
    def __init__(self):
        self.time_heap = []
        self.time_map = {}
        self._current_time = None

    def add_event(self, time, callback):
        if self._current_time is not None and time <= self._current_time:
            logger.warn(
                "Adding a new event at a time ({}) <= the current time ({})".format(
                    time, self._current_time
                )
            )

        if time in self.time_map:
            self.time_map[time].append(callback)
        else:
            new_event_list = [callback]
            self.time_map[time] = new_event_list
            heapq.heappush(self.time_heap, (time, new_event_list))

    def __len__(self):
        return len(self.time_heap)

    def __iter__(self):
        return self

    def __next__(self):
        try:
            time, event_list = heapq.heappop(self.time_heap)
            self.time_map.pop(time)
            self._current_time = time  # used for warning messages in `add_event`
            return time, event_list
        except (IndexError, KeyError):
            raise StopIteration()


class Simulation(object):
    def __init__(self, event_list, job_map):
        self.event_list = event_list
        self.job_map = job_map
        self.current_time = 0

    def add_event(self, time, callback):
        self.event_list.add_event(time, callback)

    def add_job(self, job):
        self.job_map[job.jobid] = job

    def get_job(self, jobid):
        return self.job_map[jobid]

    def advance(self):
        try:
            self.current_time, events_at_time = next(self.event_list)
        except StopIteration:
            logger.info("No more events in event list, ending simulation")
            flux_handle.reactor_stop(flux_handle.get_reactor())
        logger.debug("Fast-forwarding time to {}".format(self.current_time))
        for event in events_at_time:
            event()


def datetime_to_epoch(dt):
    return int((dt - datetime(1970, 1, 1)).total_seconds())


re_dhms = re.compile(r"^\s*(\d+)[:-](\d+):(\d+):(\d+)\s*$")
re_hms = re.compile(r"^\s*(\d+):(\d+):(\d+)\s*$")


def walltime_str_to_timedelta(walltime_str):
    (days, hours, mins, secs) = (0, 0, 0, 0)
    match = re_dhms.search(walltime_str)
    if match:
        days = int(match.group(1))
        hours = int(match.group(2))
        mins = int(match.group(3))
        secs = int(match.group(4))
    else:
        match = re_hms.search(walltime_str)
        if match:
            hours = int(match.group(1))
            mins = int(match.group(2))
            secs = int(match.group(3))
    return timedelta(days=days, hours=hours, minutes=mins, seconds=secs)


@six.add_metaclass(ABCMeta)
class JobTraceReader(object):
    def __init__(self, tracefile):
        self.tracefile = tracefile

    @abstractmethod
    def validate_trace(self):
        pass

    @abstractmethod
    def read_trace(self):
        pass


def job_from_slurm_row(row):
    kwargs = {}
    if "ExitCode" in row:
        kwargs["exitcode"] = "ExitCode"

    submit_time = datetime_to_epoch(
        datetime.strptime(row["Submit"], "%Y-%m-%dT%H:%M:%S")
    )
    elapsed = walltime_str_to_timedelta(row["Elapsed"]).total_seconds()
    timelimit = walltime_str_to_timedelta(row["Timelimit"]).total_seconds()
    if elapsed > timelimit:
        logger.warn(
            "Elapsed time ({}) greater than Timelimit ({})".format(elapsed, timelimit)
        )
    nnodes = int(row["NNodes"])
    ncpus = int(row["NCPUS"])
    if nnodes > ncpus:
        logger.warn(
            "Number of Nodes ({}) greater than Number of CPUs ({}), setting NCPUS = NNodes".format(
                nnodes, ncpus
            )
        )
        ncpus = nnodes
    elif ncpus % nnodes != 0:
        old_ncpus = ncpus
        ncpus = math.ceil(ncpus / nnodes) * nnodes
        logger.warn(
            "Number of Nodes ({}) does not evenly divide the Number of CPUs ({}), setting NCPUS to an integer multiple of the number of nodes ({})".format(
                nnodes, old_ncpus, ncpus
            )
        )

    return Job(nnodes, ncpus, submit_time, elapsed, timelimit, **kwargs)


class SacctReader(JobTraceReader):
    required_fields = ["Elapsed", "Timelimit", "Submit", "NNodes", "NCPUS"]

    def validate_trace(self):
        with open(self.tracefile) as infile:
            reader = csv.reader(infile, delimiter="|")
            header_fields = set(next(reader))
        for req_field in SacctReader.required_fields:
            if req_field not in header_fields:
                raise ValueError("Job file is missing '{}'".format(req_field))

    def read_trace(self):
        """
        You can obtain the necessary information from the sacct command using the -o flag.
        For example: sacct -o nnodes,ncpus,timelimit,state,submit,elapsed,exitcode
        """
        with open(self.tracefile) as infile:
            reader = csv.DictReader(infile, delimiter="|")
            jobs = [job_from_slurm_row(row) for row in reader]
        return jobs


def insert_resource_data(num_ranks, cores_per_rank):
    """
    Populate the KVS with the resource data of the simulated system
    An example of the data format: {"0": {"Package": 7, "Core": 7, "PU": 7, "cpuset": "0-6"}}
    """
    if num_ranks <= 0:
        raise ValueError("Requires at least one rank")

    kvs_key = "resource.hwloc.by_rank"
    resource_dict = {}
    for rank in range(num_ranks):
        resource_dict[rank] = {}
        for key in ["Package", "Core", "PU"]:
            resource_dict[rank][key] = cores_per_rank
        resource_dict[rank]["cpuset"] = (
            "0-{}".format(cores_per_rank - 1) if cores_per_rank > 1 else "0"
        )
    put_rc = flux.kvs.put(flux_handle, kvs_key, resource_dict)
    if put_rc < 0:
        raise ValueError("Error inserting resource data into KVS, rc={}".format(put_rc))
    flux.kvs.commit(flux_handle)


def sched_idle_cb(flux_handle, watcher, msg, simulation):
    logger.debug("Received a sched idle cb")
    logger.debug("msg payload: {}".format(msg.payload))
    simulation.advance()


def job_state_cb(flux_handle, watcher, msg, simulation):
    logger.debug("Received a job state cb")
    logger.debug("msg payload: {}".format(msg.payload))


def get_loaded_modules():
    modules = flux_handle.rpc("cmb.lsmod").get()
    return modules["mods"]


def load_missing_modules():
    # TODO: check that necessary modules are loaded
    # if not, load them
    # return an updated list of loaded modules
    loaded_modules = get_loaded_modules()
    pass


def reload_scheduler():
    sched_module = "sched-simple"
    # Check if there is a module already loaded providing 'sched' service,
    # if so, reload that module
    for module in get_loaded_modules():
        if "sched" in module["services"]:
            sched_module = module["name"]

    logger.debug("Reloading the '{}' module".format(sched_module))
    flux_handle.rpc("cmb.rmmod", payload={"name": "sched-simple"}).get()
    path = flux.util.modfind("sched-simple")
    flux_handle.rpc("cmb.insmod", payload=json.dumps({"path": path, "args": []})).get()



logger = logging.getLogger(__name__)


@flux.util.cli_main(logger)
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("job_file")
    parser.add_argument("num_ranks", type=int)
    parser.add_argument("cores_per_rank", type=int)
    parser.add_argument("--log-level", type=int)
    args = parser.parse_args()

    if args.log_level:
        logger.setLevel(args.log_level)

    global flux_handle
    flux_handle = flux.Flux()

    simulation = Simulation(EventList(), {})
    reader = SacctReader(args.job_file)
    reader.validate_trace()
    jobs = list(reader.read_trace())
    for job in jobs:
        job.insert_apriori_events(simulation)

    load_missing_modules()
    insert_resource_data(args.num_ranks, args.cores_per_rank)
    reload_scheduler()


    # TODO: register callback as the exec system

    watchers = []
    for event_topic, cb, args in [
            ("job-state", job_state_cb, simulation),
            ("sched-idle", sched_idle_cb, simulation),
    ]:
        flux_handle.event_subscribe(event_topic)
        watcher = flux_handle.msg_watcher_create(
            cb,
            type_mask=flux.constants.FLUX_MSGTYPE_EVENT,
            topic_glob=event_topic,
            args=args,
        )
        watcher.start()
        watchers.append(watcher)
    simulation.advance()
    flux_handle.reactor_run(flux_handle.get_reactor(), 0)

    for watcher in watchers:
        watcher.stop()

if __name__ == "__main__":
    main()
